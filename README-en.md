<div align="center">

ZH | [EN](./README-en.md)

<h1>DISC-FinLLM</h1>
  
[![Generic badge](https://img.shields.io/badge/ğŸ¤—-Huggingface%20Repo-green.svg)](https://huggingface.co/ShengbinYue/DISC-LawLLM)
[![license](https://img.shields.io/github/license/modelscope/modelscope.svg)](./LICENSE)

[Demo](https://law.fudan-disc.com) | [Technical Report](https://arxiv.org/abs/2309.11325)

</div>

DISC-FinLLM is a large model in the financial field specifically designed to provide users with professional, intelligent, and comprehensive **financial consulting services** in financial scenarios. It is developed by[Fudan University Data Intelligence and Social Computing Laboratory (Fudan-DISC)](http://fudan-disc.com) developed and open source.

We will open source the following resources in this project:
* [DISC-FinLLM model parameters](https://huggingface.co/ShengbinYue/DISC-LawLLM)
* [DISC-Fin-Eval Benchmark](https://huggingface.co/ShengbinYue/DISC-LawLLM)

You can experience our DISC-FinLLM online by visiting this [link](https://fin.fudan-disc.com).



## ç›®å½•

- [Overview](#overview)
- [Model Fine-tuning](#model-fine-tuning)
- [Inference and Deployment](#inference-and-deployment)
- [DISC-Fin-Eval Benchmark](#disc-fin-eval-benchmark)
- [Acknowledgements](#acknowledgements)
- [Disclaimer](#disclaimer)
- [Citation](#citation)
- [License](#license)

## Overview

![Image](./images/model_zh.png)

<p></p>

DISC-FinLLM is a large language model in the financial field. It is a multi-expert smart financial system composed of four modules for different financial scenarios: financial consulting, financial text analysis, financial calculation, and financial knowledge retrieval and question answering. These modules showed clear advantages in four evaluations including financial NLP tasks, human test questions, data analysis and current affairs analysis, proving that DISC-FinLLM can provide strong support for a wide range of financial fields. DISC-FinLLM can help in different application scenarios and can be used to implement different functions:

* **Financial Consultation:** This module can start multiple rounds of dialogue with users on financial topics in the Chinese financial context, or explain relevant knowledge of financial majors to users. It is composed of the financial consultation instructions part of the data set. Came for training.
* **Financial Text Analysis:** This module can help users complete NLP tasks such as information extraction, sentiment analysis, text classification, and text generation on financial texts. It is trained by the financial task instructions in the data set.
* **Financial Calculation:** This module can help users complete tasks related to mathematical calculations. In addition to basic calculations such as interest rates and growth rates, it also supports statistical analysis and includes the Black-Scholes option pricing model and the EDF expected default probability model. Financial model calculations included. This module is partially trained from the financial calculation instructions in the data set.
* **Financial Knowledge Retrieval Q&A:** This module can provide users with investment advice, current affairs analysis, and policy interpretation based on financial news, research reports, and related policy documents. It is partially trained from the retrieval enhancement instructions in the dataset.




### Model effect demonstration

#### Financial Consultation

![consult_demo](./images/example_consult.gif)

#### Financial Text Analysis

![document_demo](./images/example_task.gif)

#### Financial Calculation

![tool_demo](./images/example_tool.gif)

#### Financial Knowledge Retrieval Q&A

![exam_ref_demo](./images/example_retrieval.gif)



### DISC-Fin-SFT Dataset
DISC-FinLLM is a large financial model based on the high-quality financial data set DISC-Fin-SFT we constructed and fine-tuned the LoRA instruction on the general-domain Chinese large model Baichuan-13B-Chat. DISC-Fin-SFT contains a total of about 250,000 pieces of data, divided into four sub-data sets, which are financial consulting instructions, financial task instructions, financial computing instructions, and retrieval-enhanced instructions.

![Image](./images/data_zh.png)

| Dataset | Samples | Input Length | Output Length  |
|----------------:|----------------:|------------------------------------------------------------:|-----------------------------------------------------------:|
|    Financial Consulting Instructions |             63k |                                                          26 |                                                        369 |
|    Financial Task Instructions |            110k |                                                         676 |                                                         35 |
|    Financial Computing Instructions |             57k |                                                          73 |                                                        190 |
|    Retrieval-enhanced Instructions |             20k |                                                        1031 |                                                        521 |
|    DISC-Fin-SFT |            246k |                                                         351 |                                                        198 |

#### Financial Consulting Instructions

Financial advisory directive data comes from three parts:
- This is an English financial question and answer dataset where the quality of the answers varies. Therefore, we translated all questions in FiQA into Chinese and used ChatGPT to regenerate the answers to the questions to improve the quality of this data set.
- Explanation of financial terms. We collected more than 200 professional terms in the financial field (such as leveraged buyout) from the Internet, and then used ChatGPT to generate corresponding question and answer pairs for these professional terms to train the model to understand financial terms.
- Public posting on the Economic Management Forum. We use the self-chat prompting method to guide ChatGPT to generate multiple rounds of questions and answers around the post topic.

In the process of guiding ChatGPT to generate data, we ensured that the generated questions and answers were in line with China's national conditions, stance, attitude and language style through carefully designed prompts.

#### Financial Task Instructions
Financial task order data comes from two parts:

- Financial NLP dataset. This part is based on the existing financial NLP data set and adapted from manually written prompts. We have collected more than ten open source NLP Chinese data sets, which can be divided into categories such as sentiment analysis, information extraction, text generation, text classification and translation. The distribution of this data set looks like this:


| Dataset            | Major Task Type        | Minor Task Type           | # Samples |
|--------------------|------------------------|---------------------------|-----------:|
| FPB                | Sentiment Analysis     | Sentiment Analysis        |      18690 |
| FIQA-SA            | Sentiment Analysis     | Sentiment Analysis        |          - |
| FNSC               | Sentiment Analysis     | Sentiment Analysis        |          - |
| CCKS-NEC-2022      | Imformation Extraction | Causality Extraction      |       7499 |
| SmoothNLP IEE      | Imformation Extraction | Event Extraction          |       3256 |
| SmoothNLP NHG      | Text Generation        | Text Generation           |       4642 |
| CCKS2022-event     | Text Classification    | Event Type Classification |       3578 |
| Minds14            | Text Classification    | Intent Prediction         |      59143 |
| Financial Report   | Imformation Extraction | Entity Extraction         |      61705 |
| OpenKG             | Imformation Extraction | Entity Extraction         |       7672 |
| OpenKG             | Imformation Extraction | Entity Extraction         |      67921 |
| FDDC2018           | Translation            | Terminology Translation   |        333 |
| Wealth-alpaca-lora | Text Generation     | Keyword Generation        |      41825 |

- Financial unlabeled text dataset. This is a reading comprehension data set of financial texts. We collected a total of 87k articles from Oriental Fortune Network, including financial news and industry research report summaries. Then, based on the paragraphs in these unlabeled texts, we use ChatGPT to obtain instruction pairs.

#### Financial Computing Instructions
In financial calculations, four tools, expression calculator, equation solver, normal probability table, and counter, can help models complete most calculation tasks. Each of the four tools has different calling commands, inputs and outputs. For example, the calculator command is **[Calculator(expression)â†’result]**. In this part, the purpose of building financial calculation instructions is to train the model to call these tools to solve mathematical problems when appropriate. The definitions of the four tools are shown in the table below:
| å·¥å…·åç§°     | å·¥å…·æè¿°                                   |
|--------------|--------------------------------------------|
| è¡¨è¾¾å¼è®¡ç®—å™¨ | è¾“å…¥ï¼šåˆç­‰å‡½æ•°çš„æ•°å­¦è¡¨è¾¾å¼                 |
|              | è¾“å‡ºï¼šè¡¨è¾¾å¼çš„è®¡ç®—ç»“æœï¼ˆå°æ•°è¡¨ç¤ºï¼‰         |
| æ–¹ç¨‹æ±‚è§£å™¨   | è¾“å…¥ï¼šæ–¹ç¨‹ç»„                               |
|              | è¾“å‡ºï¼šæ–¹ç¨‹ç»„çš„è§£                           |
| è®¡æ•°å™¨       | è¾“å…¥ï¼šåŒ…å«æ•°æ®æ ·æœ¬çš„æ•°ç»„                   |
|              | è¾“å‡ºï¼šæ ·æœ¬æ•°é‡                             |
| æ¦‚ç‡è¡¨       | è¾“å…¥ï¼šæ•°å­—                                 |
|              | è¾“å‡ºï¼šæ­£æ€åˆ†å¸ƒç´¯ç§¯åˆ†å¸ƒå‡½æ•°åœ¨è¿™ä¸ªæ•°å­—å¤„çš„å€¼ |

#### Retrieval-enhanced Instructions
æ£€ç´¢å¢å¼ºæŒ‡ä»¤çš„æ„é€ åˆ†ä¸ºä¸‰æ­¥ã€‚ç¬¬ä¸€æ­¥ï¼Œæˆ‘ä»¬æ ¹æ®æ–°é—»å’Œç ”æŠ¥ç­‰é‡‘èæ–‡æœ¬æ„é€ é‡‘èåˆ†æé—®é¢˜ã€‚ç¬¬äºŒæ­¥ï¼Œæˆ‘ä»¬åœ¨çŸ¥è¯†åº“ä¸­æ£€ç´¢ä¸é—®é¢˜æœ‰å…³çš„æ–‡æ¡£ï¼Œå…¶ä¸­å‚è€ƒæ–‡æ¡£æºäºæˆ‘ä»¬æ„å»ºé‡‘èçŸ¥è¯†åº“ï¼ŒåŒ…å«18kç ”æŠ¥å’Œ69ké‡‘èæ–°é—»ã€‚ç¬¬ä¸‰æ­¥ï¼Œæˆ‘ä»¬å°†é—®é¢˜å’Œå‚è€ƒèµ„æ–™ç»“åˆåœ¨ä¸€èµ·ï¼Œç”Ÿæˆé—®é¢˜çš„ç­”æ¡ˆã€‚åœ¨è¿™ä¸ªè¿‡ç¨‹ä¸­ï¼Œé—®é¢˜å’Œç­”æ¡ˆæ˜¯ç”±ChatGPTé€šè¿‡Chain-of-Retrieval (CoR) promptingæ–¹æ³•ç”Ÿæˆçš„ã€‚æœ€ç»ˆæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªç”±20kæ¡æ£€ç´¢å¢å¼ºæŒ‡ä»¤ç»„æˆçš„æ•°æ®é›†ï¼Œå…¶ä¸­çš„æŒ‡ä»¤æ¶µç›–äº†é‡‘èé¢†åŸŸä¸­ä¸»è¦çš„åˆ†æå½¢å¼ï¼ŒåŒ…æ‹¬è¡Œä¸šåˆ†æã€æ”¿ç­–åˆ†æã€æŠ•èµ„å»ºè®®ã€å…¬å¸æˆ˜ç•¥è§„åˆ’ç­‰ã€‚

æˆ‘ä»¬å¼€æºäº†éƒ¨åˆ†æ•°æ®é›†ï¼Œæ‚¨å¯ä»¥è®¿é—®è¿™ä¸ª[é“¾æ¥](https://huggingface.co/datasets/ShengbinYue/DISC-Law-SFT)ä¸‹è½½æ•°æ®é›†ã€‚


## æ¨¡å‹å¾®è°ƒ

### LoRAå¾®è°ƒ

é’ˆå¯¹é‡‘èé¢†åŸŸçš„ä¸åŒåŠŸèƒ½ï¼Œæˆ‘ä»¬é¦–å…ˆé‡‡ç”¨äº†å¤šä¸“å®¶å¾®è°ƒçš„è®­ç»ƒç­–ç•¥ã€‚æˆ‘ä»¬åœ¨ç‰¹å®šçš„å­æ•°æ®é›†ä¸Šè®­ç»ƒæ¨¡å‹çš„å„ä¸ªæ¨¡ç»„ï¼Œä½¿å®ƒä»¬å½¼æ­¤äº’ä¸å¹²æ‰°ï¼Œç‹¬ç«‹å®Œæˆä¸åŒä»»åŠ¡ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬ä»¥Baichuan-13B-Chatä¸ºåŸºåº§æ¨¡å‹ï¼Œä½¿ç”¨LoRAæ–¹æ³•é«˜æ•ˆåœ°è¿›è¡Œå‚æ•°å¾®è°ƒã€‚

![Image](./images/lora_zh.png)

é€šè¿‡æ•°æ®é›†çš„å››ä¸ªéƒ¨åˆ†ï¼Œåˆ†åˆ«è®­ç»ƒ4ä¸ªLoRAä¸“å®¶æ¨¡ç»„ã€‚éƒ¨ç½²æ—¶ï¼Œç”¨æˆ·åªéœ€æ›´æ¢åœ¨å½“å‰åŸºåº§ä¸Šçš„LoRAå‚æ•°å°±å¯ä»¥åˆ‡æ¢åŠŸèƒ½ã€‚å› æ­¤ç”¨æˆ·èƒ½å¤Ÿæ ¹æ®ä½¿ç”¨éœ€æ±‚æ¿€æ´»/åœç”¨æ¨¡å‹çš„ä¸åŒæ¨¡ç»„ï¼Œè€Œæ— éœ€é‡æ–°åŠ è½½æ•´ä¸ªæ¨¡å‹ã€‚4ä¸ªLoRAä¸“å®¶æ¨¡ç»„åˆ†åˆ«å¦‚ä¸‹ï¼š
- é‡‘èé¡¾é—®ï¼šè¯¥æ¨¡å‹ç”¨äºå¤šè½®å¯¹è¯ã€‚ç”±äºæˆ‘ä»¬çš„é‡‘èå’¨è¯¢æŒ‡ä»¤æ•°æ®ååˆ†ä¸°å¯Œï¼Œè¯¥æ¨¡å‹å¯ä»¥åœ¨ä¸­å›½çš„é‡‘èè¯­å¢ƒä¸‹åšå‡ºé«˜è´¨é‡çš„å›ç­”ï¼Œä¸ºç”¨æˆ·è§£ç­”é‡‘èé¢†åŸŸçš„ä¸“ä¸šé—®é¢˜ï¼Œæä¾›ä¼˜è´¨çš„å’¨è¯¢æœåŠ¡ã€‚
- æ–‡ä»¶åˆ†æå¸ˆï¼šè¯¥æ¨¡å‹ä¸»è¦ç”¨äºå¤„ç†é‡‘èè‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸå†…çš„å„ç§ä»»åŠ¡ï¼ŒåŒ…æ‹¬ä½†ä¸é™äºé‡‘èæ–‡æœ¬ä¸­çš„ä¿¡æ¯æŠ½å–ã€æƒ…ç»ªåˆ†æç­‰ã€‚
- è´¢åŠ¡ä¼šè®¡å¸ˆï¼šDISC-FinLLMæ”¯æŒå››ç§å·¥å…·ï¼Œå³è¡¨è¾¾å¼è®¡ç®—å™¨ã€æ–¹ç¨‹æ±‚è§£å™¨ã€è®¡æ•°å™¨å’Œæ¦‚ç‡è¡¨ã€‚è¿™äº›å·¥å…·æ”¯æŒæˆ‘ä»¬çš„æ¨¡å‹å®Œæˆé‡‘èé¢†åŸŸçš„å¤§å¤šæ•°çš„è®¡ç®—ä»»åŠ¡ï¼Œå¦‚é‡‘èæ•°å­¦å»ºæ¨¡ã€ç»Ÿè®¡åˆ†æç­‰ã€‚å½“æ¨¡å‹éœ€è¦ä½¿ç”¨å·¥å…·æ—¶ï¼Œå®ƒå¯ä»¥ç”Ÿæˆå·¥å…·è°ƒç”¨å‘½ä»¤ï¼Œç„¶åä¸­æ–­è§£ç ï¼Œå¹¶å°†å·¥å…·è°ƒç”¨ç»“æœæ·»åŠ åˆ°ç”Ÿæˆçš„æ–‡æœ¬ä¸­ã€‚è¿™æ ·ï¼ŒDISC-FinLLMå°±å¯ä»¥å€ŸåŠ©å·¥å…·æä¾›çš„å‡†ç¡®è®¡ç®—ç»“æœï¼Œå›ç­”é‡‘èä¸­çš„è®¡ç®—é—®é¢˜ã€‚
- æ—¶äº‹åˆ†æå¸ˆï¼šæˆ‘ä»¬åœ¨ç¬¬å››ä¸ªLoRAè®­ç»ƒä¸­å¼•å…¥æ£€ç´¢æ’ä»¶ã€‚DISC-FinLLMä¸»è¦å‚è€ƒäº†ä¸‰ç±»é‡‘èæ–‡æœ¬ï¼šæ–°é—»ã€æŠ¥å‘Šå’Œæ”¿ç­–ã€‚å½“ç”¨æˆ·é—®åŠæ—¶äº‹ã€è¡Œä¸šè¶‹åŠ¿æˆ–é‡‘èæ”¿ç­–ç­‰å¸¸è§é‡‘èè¯é¢˜æ—¶ï¼Œæˆ‘ä»¬çš„æ¨¡å‹å¯ä»¥æ£€ç´¢ç›¸å…³æ–‡ä»¶ï¼Œå¹¶åƒé‡‘èä¸“å®¶ä¸€æ ·å±•å¼€åˆ†æå¹¶æä¾›å»ºè®®ã€‚

**æ‚¨å¯ä»¥ç›´æ¥ä» [Hugging Face](https://huggingface.co/Go4miii/DISC-FinLLM) ä¸Šä¸‹è½½æˆ‘ä»¬çš„LoRAæ¨¡å‹æƒé‡ã€‚**

### å…¨é‡å¾®è°ƒ

æˆ‘ä»¬ä»¥Baichuan-13B-Chatä¸ºåŸºåº§æ¨¡å‹ï¼Œæ··åˆäº†æ‰€æœ‰æ•°æ®ï¼Œåœ¨ 8 * Nvidia A800 80 GB + deepspeed çš„ç¯å¢ƒä¸‹è¿›è¡Œäº†å…¨é‡å¾®è°ƒæµ‹è¯•ã€‚

**æ‚¨å¯ä»¥ç›´æ¥ä» [Hugging Face](https://huggingface.co/Go4miii/DISC-FinLLM) ä¸Šä¸‹è½½æˆ‘ä»¬çš„å…¨å‚æ¨¡å‹æƒé‡ã€‚**


## æ¨ç†å’Œéƒ¨ç½²

å½“å‰ç‰ˆæœ¬çš„ DISC-FinLLM æ˜¯åŸºäº[Baichuan-13B-Chat](https://github.com/baichuan-inc/Baichuan-13B)è®­ç»ƒå¾—åˆ°çš„ã€‚æˆ‘ä»¬åˆ†åˆ«ä½¿ç”¨ä¸åŒæ•°æ®è¿›è¡Œäº†LoRAè®­ç»ƒï¼Œä»¥åŠä½¿ç”¨å…¨éƒ¨æ•°æ®è¿›è¡Œäº†å…¨å‚è®­ç»ƒã€‚æ‚¨å¯ä»¥ç›´æ¥ä» [Hugging Face](https://huggingface.co/Go4miii/DISC-FinLLM) ä¸Šä¸‹è½½æˆ‘ä»¬çš„æ¨¡å‹æƒé‡ã€‚



é¦–å…ˆï¼Œæ‚¨éœ€è¦å®‰è£…é¡¹ç›®çš„ä¾èµ–ç¯å¢ƒã€‚

```
pip install -r requirements.txt
```

### Python

#### å…¨å‚æ¨¡å‹

```python
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from transformers.generation.utils import GenerationConfig
from peft import PeftModel, PeftConfig

model_path = "Go4miii/DISC-FinLLM"
model = AutoModelForCausalLM.from_pretrained(
    model_path, torch_dtype=torch.float16, device_map="auto", trust_remote_code=True
)
model.generation_config = GenerationConfig.from_pretrained(model_path)
tokenizer = AutoTokenizer.from_pretrained(
    model_path, use_fast=False, trust_remote_code=True,
)

messages = [
    {"role": "user", "content": "è¯·è§£é‡Šä¸€ä¸‹ä»€ä¹ˆæ˜¯é“¶è¡Œä¸è‰¯èµ„äº§ï¼Ÿ"},
]
response = model.chat(tokenizer, messages)
print(response)
```
#### LoRAæ¨¡å‹

```python
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from transformers.generation.utils import GenerationConfig
from peft import PeftModel, PeftConfig

model_path = "baichuan-inc/Baichuan-13B-Chat"
model = AutoModelForCausalLM.from_pretrained(
    model_path, torch_dtype=torch.float16, device_map="auto", trust_remote_code=True
)
model.generation_config = GenerationConfig.from_pretrained(model_path)
tokenizer = AutoTokenizer.from_pretrained(
    model_path, use_fast=False, trust_remote_code=True,
)
model = PeftModel.from_pretrained(model, lora_path)

messages = [
    {"role": "user", "content": "è¯·è§£é‡Šä¸€ä¸‹ä»€ä¹ˆæ˜¯é“¶è¡Œä¸è‰¯èµ„äº§ï¼Ÿ"},
]
response = model.chat(tokenizer, messages)
print(response)
```


### å‘½ä»¤è¡Œå·¥å…·

```
python cli_demo.py
```

### ç½‘é¡µ Demo

ä¾é  streamlit å·¥å…·è¿è¡Œä»¥ä¸‹å‘½ä»¤ï¼Œä¼šåœ¨æœ¬åœ°å¯åŠ¨ä¸€ä¸ª web æœåŠ¡ï¼ŒæŠŠæ§åˆ¶å°ç»™å‡ºçš„åœ°å€è¾“å…¥æµè§ˆå™¨å³å¯è®¿é—®ï¼š

```
streamlit run web_demo.py --server.port 8888
```

æ­¤å¤–ï¼Œç›®å‰ç‰ˆæœ¬çš„ DISC-FinLLM æ˜¯ä»¥ Baichuan-13B ä½œä¸ºåŸºåº§çš„ï¼Œæ‚¨å¯ä»¥å‚ç…§ [Baichuan-13B](https://github.com/baichuan-inc/Baichuan-13B) çš„ä»‹ç»æ¥è¿›è¡Œ int8 æˆ– int4 é‡åŒ–æ¨ç†éƒ¨ç½²ä»¥åŠ CPU éƒ¨ç½²ã€‚

<!-- ## æ¨¡å‹å¾®è°ƒ

é’ˆå¯¹é‡‘èé¢†åŸŸçš„ä¸åŒåŠŸèƒ½ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†å¤šä¸“å®¶å¾®è°ƒçš„è®­ç»ƒç­–ç•¥ã€‚æˆ‘ä»¬åœ¨ç‰¹å®šçš„å­æ•°æ®é›†ä¸Šè®­ç»ƒæ¨¡å‹çš„å„ä¸ªæ¨¡ç»„ï¼Œä½¿å®ƒä»¬å½¼æ­¤äº’ä¸å¹²æ‰°ï¼Œç‹¬ç«‹å®Œæˆä¸åŒä»»åŠ¡ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬ä½¿ç”¨DDPæŠ€æœ¯çš„Low-rank adaptionï¼ˆLoRAï¼‰æ–¹æ³•é«˜æ•ˆåœ°è¿›è¡Œå‚æ•°å¾®è°ƒã€‚

å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬ä»¥Baichuan-13Bä¸ºåŸºåº§æ¨¡å‹ï¼Œé€šè¿‡æ•°æ®é›†çš„å››ä¸ªéƒ¨åˆ†ï¼Œåˆ†åˆ«è®­ç»ƒ4ä¸ªLoRAä¸“å®¶æ¨¡ç»„ï¼Œå¦‚å›¾12æ‰€ç¤ºã€‚éƒ¨ç½²æ—¶ï¼Œç”¨æˆ·åªéœ€æ›´æ¢åœ¨å½“å‰åŸºåº§ä¸Šçš„LoRAå‚æ•°å°±å¯ä»¥åˆ‡æ¢åŠŸèƒ½ã€‚å› æ­¤ç”¨æˆ·èƒ½å¤Ÿæ ¹æ®ä½¿ç”¨éœ€æ±‚æ¿€æ´»/åœç”¨æ¨¡å‹çš„ä¸åŒæ¨¡ç»„ï¼Œè€Œæ— éœ€é‡æ–°åŠ è½½æ•´ä¸ªæ¨¡å‹ã€‚4ä¸ªLoRAä¸“å®¶æ¨¡ç»„åˆ†åˆ«å¦‚ä¸‹ï¼š
- é‡‘èé¡¾é—®ï¼šè¯¥æ¨¡å‹ç”¨äºå¤šè½®å¯¹è¯ã€‚ç”±äºæˆ‘ä»¬çš„é‡‘èå’¨è¯¢æŒ‡ä»¤æ•°æ®ååˆ†ä¸°å¯Œï¼Œè¯¥æ¨¡å‹å¯ä»¥åœ¨ä¸­å›½çš„é‡‘èè¯­å¢ƒä¸‹åšå‡ºé«˜è´¨é‡çš„å›ç­”ï¼Œä¸ºç”¨æˆ·è§£ç­”é‡‘èé¢†åŸŸçš„ä¸“ä¸šé—®é¢˜ï¼Œæä¾›ä¼˜è´¨çš„å’¨è¯¢æœåŠ¡ã€‚
- æ–‡ä»¶åˆ†æå¸ˆï¼šè¯¥æ¨¡å‹ä¸»è¦ç”¨äºå¤„ç†é‡‘èè‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸå†…çš„å„ç§ä»»åŠ¡ï¼ŒåŒ…æ‹¬ä½†ä¸é™äºé‡‘èæ–‡æœ¬ä¸­çš„ä¿¡æ¯æŠ½å–ã€æƒ…ç»ªåˆ†æç­‰ã€‚
- è´¢åŠ¡ä¼šè®¡å¸ˆï¼šDISC-FinLLMæ”¯æŒå››ç§å·¥å…·ï¼Œå³è¡¨è¾¾å¼è®¡ç®—å™¨ã€æ–¹ç¨‹æ±‚è§£å™¨ã€è®¡æ•°å™¨å’Œæ¦‚ç‡è¡¨ã€‚è¿™äº›å·¥å…·æ”¯æŒæˆ‘ä»¬çš„æ¨¡å‹å®Œæˆé‡‘èé¢†åŸŸçš„å¤§å¤šæ•°çš„è®¡ç®—ä»»åŠ¡ï¼Œå¦‚é‡‘èæ•°å­¦å»ºæ¨¡ã€ç»Ÿè®¡åˆ†æç­‰ã€‚å½“æ¨¡å‹éœ€è¦ä½¿ç”¨å·¥å…·æ—¶ï¼Œå®ƒå¯ä»¥ç”Ÿæˆå·¥å…·è°ƒç”¨å‘½ä»¤ï¼Œç„¶åä¸­æ–­è§£ç ï¼Œå¹¶å°†å·¥å…·è°ƒç”¨ç»“æœæ·»åŠ åˆ°ç”Ÿæˆçš„æ–‡æœ¬ä¸­ã€‚è¿™æ ·ï¼ŒDISC-FinLLMå°±å¯ä»¥å€ŸåŠ©å·¥å…·æä¾›çš„å‡†ç¡®è®¡ç®—ç»“æœï¼Œå›ç­”é‡‘èä¸­çš„è®¡ç®—é—®é¢˜ã€‚
- æ—¶äº‹åˆ†æå¸ˆï¼šæˆ‘ä»¬åœ¨ç¬¬å››ä¸ªLoRAè®­ç»ƒä¸­å¼•å…¥æ£€ç´¢æ’ä»¶ã€‚DISC-FinLLMä¸»è¦å‚è€ƒäº†ä¸‰ç±»é‡‘èæ–‡æœ¬ï¼šæ–°é—»ã€æŠ¥å‘Šå’Œæ”¿ç­–ã€‚å½“ç”¨æˆ·é—®åŠæ—¶äº‹ã€è¡Œä¸šè¶‹åŠ¿æˆ–é‡‘èæ”¿ç­–ç­‰å¸¸è§é‡‘èè¯é¢˜æ—¶ï¼Œæˆ‘ä»¬çš„æ¨¡å‹å¯ä»¥æ£€ç´¢ç›¸å…³æ–‡ä»¶ï¼Œå¹¶åƒé‡‘èä¸“å®¶ä¸€æ ·å±•å¼€åˆ†æå¹¶æä¾›å»ºè®®ã€‚ -->


<!-- å¼€å‘è€…å¯ä»¥å¯¹ DISC-FinLLM è¿›è¡Œå¾®è°ƒä½¿ç”¨ã€‚åœ¨æ­¤å¯ä»¥å‚ç…§ä¸ DISC-LawLLM å…¼å®¹çš„å¾®è°ƒå·¥å…· [LLaMA Efficient Tuning](https://github.com/hiyouga/LLaMA-Efficient-Tuning) æˆ–æ˜¯æˆ‘ä»¬çš„ [DISC-MedLLM](https://github.com/FudanDISC/DISC-MedLLM) åŒ»ç–—å¤§æ¨¡å‹ã€‚æˆ‘ä»¬ä»¥ [LLaMA Efficient Tuning](https://github.com/hiyouga/LLaMA-Efficient-Tuning) ä¸ºä¾‹ç»™å‡º**å…¨é‡**å’Œ **LoRA** ä¸¤ç§å¾®è°ƒç¤ºä¾‹ã€‚

é¦–å…ˆï¼Œä¸‹è½½ [LLaMA Efficient Tuning](https://github.com/hiyouga/LLaMA-Efficient-Tuning) å¹¶æŒ‰å…¶è¦æ±‚[å®‰è£…ä¾èµ–](https://github.com/hiyouga/LLaMA-Efficient-Tuning#getting-started)ã€‚æ³¨æ„è®­ç»ƒæ•°æ®æŒ‰ç…§é¡¹ç›®ä¸­çš„è¦æ±‚è¿›è¡Œå¤„ç†ã€‚ä¸‹é¢æˆ‘ä»¬ç»™å‡ºä¸¤ç§å¾®è°ƒåœºæ™¯ä¸‹çš„è„šæœ¬æ ·ä¾‹ã€‚

### å…¨é‡å¾®è°ƒ

æˆ‘ä»¬åœ¨ 8 * Nvidia A800 80 GB + deepspeed çš„ç¯å¢ƒä¸‹è¿›è¡Œäº†å…¨é‡å¾®è°ƒæµ‹è¯•ã€‚è®­ç»ƒå¯åŠ¨è„šæœ¬ç¤ºä¾‹å¦‚ä¸‹ï¼š

```
deepspeed --num_gpus=8 src/train_bash.py \
    --stage sft \
    --model_name_or_path S heng bin \
    --do_train \
    --dataset alpaca_gpt4_zh \
    --template baichuan \
    --finetuning_type full \
    --output_dir path_to_your_sft_checkpoint \
    --overwrite_cache \
    --per_device_train_batch_size 4 \ 
    --per_device_eval_batch_size 4 \ 
    --gradient_accumulation_steps 8 \ 
    --preprocessing_num_workers 8 \
    --lr_scheduler_type cosine \
    --logging_steps 10 \
    --save_steps 100 \
    --eval_steps 100 \
    --learning_rate 5e-5 \
    --max_grad_norm 0.5 \
    --num_train_epochs 2.0 \
    --dev_ratio 0.01 \
    --evaluation_strategy steps \
    --load_best_model_at_end \
    --plot_loss \
    --fp16 \
    --deepspeed deepspeed.json
```

`deep_speed.json` é…ç½®ç¤ºä¾‹å¦‚ä¸‹ï¼š

```json
{
    "train_micro_batch_size_per_gpu": "auto",
    "zero_allow_untested_optimizer": true,
    "fp16": {
        "enabled": "auto",
        "loss_scale": 0,
        "initial_scale_power": 16, 
        "loss_scale_window": 1000,
        "hysteresis": 2,
        "min_loss_scale": 1
    },  
    "zero_optimization": {
        "stage": 2,
        "allgather_partitions": true,
        "allgather_bucket_size": 5e8,
        "overlap_comm": false,
        "reduce_scatter": true,
        "reduce_bucket_size": 5e8,
        "contiguous_gradients" : true
    }
}
```

### LoRA å¾®è°ƒ

æˆ‘ä»¬åœ¨ 4 * Nvidia A800 80G æ˜¾å¡ä¸Šè¿›è¡Œäº† LoRA å¾®è°ƒæµ‹è¯•ã€‚è®­ç»ƒå¯åŠ¨è„šæœ¬ç¤ºä¾‹å¦‚ä¸‹ï¼š

```
torchrun --nproc_per_node 4 src/train_bash.py \
    --stage sft \
    --model_name_or_path ShengbinYue/DISC-LawLLM \
    --do_train \
    --dataset alpaca_gpt4_zh \
    --template baichuan \
    --finetuning_type lora \
    --lora_rank 8 \ 
    --lora_target W_pack \
    --output_dir path_to_your_sft_checkpoint \
    --overwrite_cache \
    --per_device_train_batch_size 4 \ 
    --per_device_eval_batch_size 4 \ 
    --gradient_accumulation_steps 8 \ 
    --preprocessing_num_workers 16 \
    --lr_scheduler_type cosine \
    --logging_steps 10 \
    --save_steps 100 \
    --eval_steps 100 \
    --learning_rate 1e-5 \
    --max_grad_norm 0.5 \
    --num_train_epochs 2.0 \
    --dev_ratio 0.01 \
    --evaluation_strategy steps \
    --load_best_model_at_end \
    --plot_loss \
    --fp16
``` -->

## DISC-Fin-Eval-Benchmark

æˆ‘ä»¬å»ºç«‹äº†ä¸€ä¸ªå…¨é¢çš„è¯„ä¼°æ¡†æ¶ â€”â€” DISC-Fin-Eval Benchmarkï¼Œä»å„ä¸ªè§’åº¦ä¸¥æ ¼è¯„ä¼°æˆ‘ä»¬çš„æ¨¡å‹ã€‚è¯¥è¯„ä¼°æ¡†æ¶åŒ…æ‹¬å››ä¸ªä¸åŒçš„ç»„æˆéƒ¨åˆ†ï¼Œå³ï¼šé‡‘èNLPä»»åŠ¡ã€äººç±»è¯•é¢˜ã€èµ„æ–™åˆ†æå’Œæ—¶äº‹åˆ†æã€‚è¿™ä¸€è¯„ä¼°æ¡†æ¶å…¨é¢åœ°è¯æ˜äº†æˆ‘ä»¬æ¨¡å‹èƒ½åŠ›å’Œè®­ç»ƒæ•°æ®çš„æœ‰æ•ˆæ€§ã€‚æ‚¨å¯ä»¥ç‚¹å‡»æ­¤[é“¾æ¥](./eval)ä½¿ç”¨æˆ‘ä»¬çš„ DISC-Fin-Eval-Benchmarkã€‚

### è¯„æµ‹ç³»ç»Ÿ

#### é‡‘èNLPä»»åŠ¡è¯„æµ‹
æˆ‘ä»¬ä½¿ç”¨FinCUGEè¯„ä¼°åŸºå‡†æµ‹è¯•æ¨¡å‹å¤„ç†é‡‘èNLPä»»åŠ¡çš„èƒ½åŠ›ã€‚è¿™ä¸ªè¯„æµ‹ä¸€å…±åŒ…å«å…«é¡¹ä»»åŠ¡ï¼Œå…¶ä¸­åŒ…æ‹¬æƒ…æ„Ÿåˆ†æã€å…³ç³»æŠ½å–ã€æ–‡æœ¬æ‘˜è¦ã€æ–‡æœ¬åˆ†ç±»ã€äº‹ä»¶æŠ½å–å’Œå…¶ä»–ä»»åŠ¡ã€‚æˆ‘ä»¬é€šè¿‡æç¤ºæ¨¡æ¿å°†è¿™ä¸ªæ•°æ®é›†æ”¹é€ ä¸ºå°æ ·æœ¬ï¼ˆfew-shotï¼‰å½¢å¼ï¼Œä½¿ç”¨å¸¸ç”¨çš„å‡†ç¡®åº¦ï¼ˆaccuracyï¼‰ã€F1å’ŒRougeæŒ‡æ ‡è¯„ä»·æ¨¡å‹çš„è¡¨ç°ï¼Œæ¥è¡¡é‡æ¨¡å‹åœ¨é‡‘èé¢†åŸŸä¸­ç†è§£æ–‡æœ¬å’Œç”Ÿæˆç›¸å…³å›ç­”çš„èƒ½åŠ›ã€‚è¯„æµ‹ç»“æœï¼ˆ%ï¼‰å¦‚ä¸‹ï¼š
|  æ¨¡å‹   â†“  --è¯„æµ‹é›† â†’  | FinFE (Accuracy) | FinQA (F1) | FinCQA (F1) | FinNA (ROUGE) | FinRE (F1) | FinESE (F1) | å¹³å‡å€¼ |
|:-----------------:|:----------------:|:----------:|:-----------:|:-------------:|:----------:|:-----------:|:------:|
| Baichuan-13B-Chat |       64.8       |    38.1    |     33.6    |      31.0     |     9.1    |     18.6    |  31.0  |
|            (LoRA) |       69.3       |    42.4    |     42.0    |      30.9     |    10.1    |     45.3    |  40.0  |
|           ChatGLM |       56.7       |    31.8    |     35.1    |      32.5     |    13.0    |     48.7    |  36.3  |
|            (LoRA) |       60.7       |    41.4    |     36.4    |      34.7     |    10.7    |     46.2    |  38.4  |
|          ChatGLM2 |       61.3       |    28.8    |     35.9    |      28.9     |    11.7    |     42.1    |  34.8  |
|            (LoRA) |       65.3       |    37.6    |     36.4    |      33.4     |    11.8    |     39.5    |  37.3  |

**ä½ å¯ä»¥åœ¨è¿™é‡ŒæŸ¥çœ‹æˆ‘ä»¬[é‡‘èNLPä»»åŠ¡è¯„æµ‹](https://github.com/FudanDISC/DISC-FinLLM/tree/main/eval/evaluator)çš„å…·ä½“å†…å®¹ã€‚**

#### äººç±»è¯•é¢˜è¯„æµ‹
æˆ‘ä»¬ä½¿ç”¨äº†FIN-EvalåŸºå‡†è¯„ä¼°æ¨¡å‹åœ¨å›ç­”çœŸäººç”Ÿæˆçš„é‡‘èé—®é¢˜ä¸Šçš„èƒ½åŠ›ï¼Œè¿™ä¸ªåŸºå‡†æ¶µç›–äº†é‡‘èã€ç»æµã€ä¼šè®¡ã€è¯ä¹¦ç­‰å­¦ç§‘çš„é«˜è´¨é‡å¤šé¡¹é€‰æ‹©é¢˜ã€‚æˆ‘ä»¬ä»¥å‡†ç¡®åº¦ä¸ºæŒ‡æ ‡ï¼Œæ¥è¡¡é‡æ¨¡å‹çš„è¡¨ç°ã€‚è¯„æµ‹ç»“æœï¼ˆ%ï¼‰å¦‚ä¸‹ï¼š
| æ¨¡å‹                     | é‡‘è | ç»æµ | ä¼šè®¡ | è¯ä¹¦ | å¹³å‡å€¼ |
|--------------------------|-----:|-----:|-----:|-----:|-------:|
| GPT-4                    | 71.0 | 74.5 | 59.3 | 70.4 |   68.6 |
| ChatGPT                  | 59.3 | 61.6 | 45.2 | 55.1 |   55.0 |
| Baichuan-13B-Base        | 52.6 | 50.2 | 43.4 | 53.5 |   50.1 |
| Baichuan-13B-Chat        | 51.6 | 51.1 | 41.7 | 52.8 |   49.4 |
| ChatGLM2-6B              | 46.5 | 46.4 | 44.5 | 51.5 |   47.4 |
| InternLM-7B              | 49.0 | 49.2 | 40.5 | 49.4 |   47.1 |
| InternLM-Chat-7B         | 48.4 | 49.1 | 40.8 | 49.5 |   47.0 |
| LLaMA-2-Chat-70B         | 47.1 | 46.7 | 41.5 | 45.7 |   45.2 |
| FinGPT-v3-6B             | 50.5 | 42.5 | 50.8 | 52.1 |   49.6 |
| DISC-FinLLM ï¼ˆé‡‘èå’¨è¯¢ï¼‰ | 54.4 | 45.4 | 52.8 | 51.8 |   51.6 |
| DISC-FinLLM ï¼ˆé‡‘èä»»åŠ¡ï¼‰ | 57.4 | 48.8 | 49.5 | 49.7 |   51.5 |
| DISC-FinLLM ï¼ˆæ£€ç´¢å¢å¼ºï¼‰ | 56.1 | 44.0 | 49.5 | 50.6 |   50.6 |
| DISC-FinLLM ï¼ˆé‡‘èè®¡ç®—ï¼‰ | 54.8 | 50.2 | 46.9 | 50.6 |   50.9 |
| DISC-FinLLM ï¼ˆå…¨æ•°æ®ï¼‰   | 53.8 | 47.9 | 42.0 | 49.1 |   48.7 |


#### èµ„æ–™åˆ†æè¯„æµ‹
æˆ‘ä»¬æ‰‹åŠ¨æ„é€ äº†ä¸€ä¸ªç”±100ä¸ªè´¢ç»è®¡ç®—é¢˜ç»„æˆçš„æ•°æ®é›†ï¼Œç”¨äºè¯„ä¼°æ¨¡å‹åœ¨è®¡ç®—ä»»åŠ¡ä¸­çš„èƒ½åŠ›ã€‚è¿™äº›æµ‹è¯„é—®é¢˜æ”¹ç¼–è‡ªä¸­å›½è¡Œæ”¿èŒä¸šèƒ½åŠ›æµ‹éªŒä¸­çš„ææ–™åˆ†æè®¡ç®—é¢˜ï¼ŒåŒ…æ‹¬è®¡ç®—åŒæ¯”å¢é•¿ç‡å’Œäº§å€¼æ¯”ä¾‹ç­‰ã€‚æˆ‘ä»¬æ ¹æ®æ¨¡å‹ç»™å‡ºè®¡ç®—å…¬å¼å’Œè®¡ç®—ç»“æœçš„æ­£ç¡®ç‡æ¥è¯„ä¼°æ¨¡å‹çš„è¡¨ç°ã€‚è¯„æµ‹ç»“æœå¦‚ä¸‹ï¼š
|                          | è®¡ç®—å…¬å¼ | è®¡ç®—å…¬å¼ä¸ç»“æœ |
|--------------------------|:--------:|:--------------:|
| GPT-3.5-turbo            |   0.28   |      0.26      |
| Baichuan-13B-Chat        |   0.20   |      0.12      |
| DISC-FinLLM ï¼ˆé‡‘èè®¡ç®—ï¼‰ |   0.35   |      0.35      |


#### æ—¶äº‹åˆ†æè¯„æµ‹
æ­¤è¯„æµ‹åŸºäºGPT-4æ¨¡å‹ä½œå‡ºè¯„ä¼°ã€‚æˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªé‡‘èé—®é¢˜æ•°æ®é›†ï¼Œå…¶ä¸­çš„é—®é¢˜éœ€è¦æ¨¡å‹ä½¿ç”¨æœ€æ–°ä¿¡æ¯æ¥è·å¾—å‡†ç¡®ç­”æ¡ˆã€‚ç„¶åæˆ‘ä»¬åœ¨è°·æ­Œç­‰æœç´¢å¼•æ“ä¸­æ‰‹åŠ¨æœç´¢ï¼Œä»¥æ”¶é›†ä¸æ¯ä¸ªé—®é¢˜ç›¸å…³çš„å¤šä¸ªå‚è€ƒæ–‡æ®µã€‚è¯¥æ•°æ®é›†æ—¨åœ¨è¯„ä¼°å‡ºæ¨¡å‹åœ¨å›ç­”é‡‘èé—®é¢˜æ—¶æ£€ç´¢ä¿¡æ¯çš„ç›¸å…³æ€§å’Œå‡†ç¡®æ€§ï¼Œæˆ‘ä»¬ç”¨å››ä¸ªæŒ‡æ ‡è¯„ä»·æ¨¡å‹çš„è¡¨ç°ï¼Œå³å‡†ç¡®æ€§ã€å®ç”¨æ€§ã€è¯­è¨€è´¨é‡å’Œæ€è€ƒæ€§ã€‚è¯„æµ‹ç»“æœå¦‚ä¸‹ï¼š
|                          | å‡†ç¡®æ€§ | å®ç”¨æ€§ | è¯­è¨€è´¨é‡ | æ€è€ƒæ€§ |
|--------------------------|:------:|:------:|:--------:|:------:|
| Baichuan13B-Chat         |  4.08  |  4.15  |   4.21   |  3.88  |
| DISC-FinLLM ï¼ˆæ£€ç´¢å¢å¼ºï¼‰ |  4.13  |  4.29  |   4.33   |  3.95  |


**ä½ å¯ä»¥åœ¨è¿™é‡ŒæŸ¥çœ‹æˆ‘ä»¬[èµ„æ–™åˆ†æè¯„æµ‹](https://github.com/FudanDISC/DISC-FinLLM/tree/main/eval/computing_eval.json)ã€[æ—¶äº‹åˆ†æè¯„æµ‹](https://github.com/FudanDISC/DISC-FinLLM/tree/main/eval/retriever_eval.json)çš„æ•°æ®é›†ã€‚**

<!-- ### ä¸»è§‚è¯„æµ‹

åœ¨ä¸»è§‚è¯„æµ‹éƒ¨åˆ†ï¼Œæˆ‘ä»¬é‡‡ç”¨é—®ç­”é¢˜å½¢å¼è¿›è¡Œè¯„ä¼°ï¼Œæ¨¡æ‹Ÿä¸»è§‚è€ƒè¯•é—®é¢˜çš„è¿‡ç¨‹ã€‚æˆ‘ä»¬ä»æ³•å¾‹å’¨è¯¢ã€åœ¨çº¿è®ºå›ã€ä¸å¸æ³•ç›¸å…³çš„å‡ºç‰ˆç‰©å’Œæ³•å¾‹æ–‡ä»¶ä¸­æ‰‹å·¥æ„å»ºäº†ä¸€ä¸ªé«˜è´¨é‡çš„æµ‹è¯•é›†ã€‚æˆ‘ä»¬ç”¨ GPT-3.5 Turbo ä½œä¸ºè£åˆ¤æ¨¡å‹æ¥è¯„ä¼°æ¨¡å‹çš„è¾“å‡ºï¼Œå¹¶åŸºäºæ ‡å‡†ç­”æ¡ˆç”¨å‡†ç¡®æ€§ã€å®Œæ•´æ€§å’Œæ¸…æ™°åº¦è¿™ä¸‰ä¸ªæ ‡å‡†æä¾› 1-5 çš„è¯„åˆ†ã€‚

ä¸»è§‚é¢˜æ•°æ®é›†ä»æ¥æºäºæ³•å¾‹å’¨è¯¢ã€ç½‘ä¸Šå‘å¸–ã€å¸æ³•ç›¸å…³å‡ºç‰ˆç‰©å’Œæ³•å¾‹æ–‡ä¹¦ä¸­æ‰‹åŠ¨æ„å»ºçš„ä¸€ä¸ªé«˜è´¨é‡çš„æµ‹è¯•é›†ï¼Œå…¶ä¸­åŒ…æ‹¬ 300 ä¸ªç¤ºä¾‹ï¼Œæ¶µç›–äº†æ³•å¾‹çŸ¥è¯†é—®ç­”ã€æ³•å¾‹å’¨è¯¢å’Œåˆ¤å†³é¢„æµ‹ç­‰åœºæ™¯ã€‚

**ä½ å¯ä»¥åœ¨è¿™é‡ŒæŸ¥çœ‹æˆ‘ä»¬çš„[ä¸»è§‚è¯„æµ‹é›†](https://github.com/FudanDISC/DISC-LawLLM/tree/main/eval/data/subjective_eval)** -->

<!-- ### è¯„æµ‹ç»“æœ

å®¢è§‚é¢˜è¯„æµ‹é‡‡ç”¨ few-shot æ–¹å¼ï¼Œç»“æœï¼ˆ%ï¼‰å¦‚ä¸‹ï¼š

|        æ¨¡å‹        |  NJE å•é€‰   |  NJE å¤šé€‰   |  PAE å•é€‰   |  PAE å¤šé€‰   |  CPA å•é€‰   |  CPA å¤šé€‰   | UNGEE å•é€‰  | UNGEE å¤šé€‰  |  PFE å•é€‰   |  LBK å•é€‰   |   å¹³å‡   |
|:----------------:|:---------:|:---------:|:---------:|:---------:|:---------:|:---------:|:---------:|:---------:|:---------:|:---------:|:---------:|
|     ChatGLM      |   31.66   |   1.08    |   27.97   |   2.90    |   37.06   |   13.33   |   39.69   |   20.69   |   37.65   |   42.91   |   24.66   |
|  Baichuan-Chat   |   31.47   |   10.15   |   29.66   |   8.70    |   35.53   |   19.17   |   50.00   |   27.59   |   53.12   |   53.45   |   30.78   |
| Chinese-Alpaca-2 |   25.70   |   10.15   |   30.51   |   11.59   |   32.99   |   19.17   |   40.94   |   21.84   |   44.12   |   43.27   |   26.73   |
|  GPT-3.5-turbo   |   36.50   |   10.58   |   37.29   |   17.03   | **42.13** | **21.67** | **51.25** | **28.74** |   53.53   |   54.18   |   34.10   |
|     LexiLaw      |   20.11   |   7.56    |   23.73   |   10.14   |   24.87   |   19.17   |   31.56   |   16.09   |   31.76   |   40.36   |   21.50   |
|      LawGPT      |   22.91   |   6.26    |   31.36   |   7.61    |   25.38   |   16.67   |   30.31   |   13.79   |   34.71   |   29.09   |   20.60   |
|   Lawyer LLaMa   |   35.75   |   5.62    |   32.20   |   6.52    |   29.95   |   13.33   |   32.50   |   14.94   |   39.41   |   39.64   |   25.05   |
|     ChatLaw      |   27.56   |   7.99    |   31.36   |   9.42    |   35.53   |   11.67   |   35.62   |   17.24   |   42.35   |   41.09   |   25.20   |
|   DISC-LawLLM    | **42.09** | **19.87** | **40.68** | **18.48** |   39.59   |   19.17   |   50.94   |   25.29   | **57.06** | **54.91** | **37.10** |

ä¸»è§‚é¢˜è¯„æµ‹åˆ†æ•°ä¸º 1-5ï¼Œç»“æœå¦‚ä¸‹ï¼š

|        æ¨¡å‹        | å‡†ç¡®æ€§  | å®Œæ•´æ€§  | æ¸…æ™°æ€§  |  å¹³å‡  |
|:----------------:|:----:|:----:|:----:|:----:|
|     ChatGLM      | 2.64 | 2.75 | 3.23 | 2.87 |
|  Baichuan-Chat   | 3.22 | **3.34** | 3.18 | 3.25 |
| Chinese-Alpaca-2 | 3.13 | 3.23 | 3.17 | 3.17 |
|     LexiLaw      | 3.06 | 2.62 | 3.00 | 2.90 |
|      LawGPT      | 3.02 | 2.58 | 2.96 | 2.86 |
|   Lawyer LLaMa   | 3.13 | 2.83 | 3.35 | 3.10 |
|     ChatLaw      | 3.31 | 2.90 | 3.35 | 3.19 |
|   DISC-LawLLM    | **3.46** | 3.12 | **3.59** | **3.39** | -->

## è‡´è°¢

æœ¬é¡¹ç›®åŸºäºå¦‚ä¸‹å¼€æºé¡¹ç›®å±•å¼€ï¼Œåœ¨æ­¤å¯¹ç›¸å…³é¡¹ç›®å’Œå¼€å‘äººå‘˜è¡¨ç¤ºè¯šæŒšçš„æ„Ÿè°¢ï¼š

- [**Baichuan-13B**](https://github.com/baichuan-inc/Baichuan-13B)
- [**Langchain-Chatchat**](https://github.com/chatchat-space/Langchain-Chatchat)
- [**LLaMA Efficient Tuning**](https://github.com/hiyouga/LLaMA-Efficient-Tuning)
- [**FireFly**](https://github.com/yangjianxin1/Firefly)
- [**FinEval**](https://github.com/SUFE-AIFLM-Lab/FinEval)

åŒæ ·æ„Ÿè°¢å…¶ä»–é™äºç¯‡å¹…æœªèƒ½åˆ—ä¸¾çš„ä¸ºæœ¬é¡¹ç›®æä¾›äº†é‡è¦å¸®åŠ©çš„å·¥ä½œã€‚

## å£°æ˜

DISC-FinLLM æœ‰ç€ç›®å‰å¤§è¯­è¨€æ¨¡å‹å°šæ— æ³•å…‹æœçš„é—®é¢˜å’Œç¼ºé™·ï¼Œå°½ç®¡å®ƒèƒ½å¤Ÿåœ¨è®¸å¤šä»»åŠ¡å’Œæƒ…å¢ƒä¸Šæä¾›é‡‘èé¢†åŸŸçš„æœåŠ¡ï¼Œä½†æ¨¡å‹åº”å½“ä»…ä¾›ç”¨æˆ·å‚è€ƒä½¿ç”¨ï¼Œå¹¶ä¸èƒ½æ›¿ä»£ä¸“ä¸šé‡‘èåˆ†æå¸ˆå’Œé‡‘èä¸“å®¶ï¼Œæˆ‘ä»¬å¸Œæœ›ä½¿ç”¨ DISC-FinLLM çš„ç”¨æˆ·ä»¥æ‰¹åˆ¤æ€§çš„çœ¼å…‰å»è¯„ä¼°æ¨¡å‹ã€‚æˆ‘ä»¬ä¸å¯¹å› ä½¿ç”¨ DISC-FinLLM æ‰€å¼•å‘çš„ä»»ä½•é—®é¢˜ã€é£é™©æˆ–ä¸è‰¯åæœæ‰¿æ‹…è´£ä»»ã€‚

## å¼•ç”¨

å¦‚æœæˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨çš„ç ”ç©¶å’Œå·¥ä½œæœ‰å¸®åŠ©ï¼Œè¯·å¦‚ä¸‹å¼•ç”¨æˆ‘ä»¬çš„é¡¹ç›®ï¼š

```
@misc{yue2023disclawllm,
    title={DISC-LawLLM: Fine-tuning Large Language Models for Intelligent Legal Services}, 
    author={Shengbin Yue and Wei Chen and Siyuan Wang and Bingxuan Li and Chenchen Shen and Shujun Liu and Yuxuan Zhou and Yao Xiao and Song Yun and Xuanjing Huang and Zhongyu Wei},
    year={2023},
    eprint={2309.11325},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}
```

## åè®®

DISC-FinLLM å¯åœ¨ Apache è®¸å¯è¯ä¸‹ä½¿ç”¨ã€‚è¯·æŸ¥çœ‹ [LICENSE](./LICENSE) æ–‡ä»¶è·å–æ›´å¤šä¿¡æ¯ã€‚


